{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPev+bWzcMp8YrvRhs4l6Oh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itcxx/Notebook/blob/main/RI_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 强化学习 - 第二讲 基础概念\n",
        "\n",
        "> 本节介绍强化学习的相关概念\n",
        "\n",
        "* 马尔可夫决策（Markov decision process,MDP)\n",
        "* \n",
        "\n",
        "\n",
        "> ##### 智能体(agent)\n",
        "\n",
        "强化学习的主体被称为智能体 (agent)。通俗地说，由谁做动作或决策，谁就是智能\n",
        "体。比如在超级玛丽游戏中，玛丽奥就是智能体。\n",
        "\n",
        "> ##### 环境(environment)\n",
        "\n",
        "\n",
        "环境（environment）是与智能体交互的对象，可以抽象地理解为交互过程中的规\n",
        "则或机理。在超级玛丽的例子中，游戏程序就是环境\n",
        "\n",
        "> ##### 状态(state)\n",
        "\n",
        "在每个时刻，环境有一个状态 (state)，可以理解为对当前时刻环境的概括。在超级\n",
        "玛丽的例子中，可以把屏幕当前的画面（或者最近几帧画面）看做状态。玩家只需要知\n",
        "道当前画面（或者最近几帧画面）就能够做出正确的决策，决定下一步是让超级玛丽向\n",
        "左、向右、或是向上。因此，状态是做决策的依据。\n",
        "\n",
        "> ##### 状态空间(state space) -S\n",
        "\n",
        "状态空间（state space）是指所有可能存在状态的集合，记作花体字母 S。状态空间\n",
        "可以是离散的，也可以是连续的。状态空间可以是有限集合，也可以是无限可数集合。\n",
        "\n",
        "> ##### 动作（action） -a\n",
        "\n",
        "动作（action）是智能体基于当前状态所做出的决策。在超级玛丽的例子中，假设玛丽奥只能向左走、向右走、向上跳\n",
        "\n",
        "\n",
        "> ##### 动作空间(action space) -A\n",
        "\n",
        "动作空间（action space）是指所有可能动作的集合，记作花体字母 A。在超级玛丽例\n",
        "子中，动作空间是 A = {左, 右, 上}。\n",
        "\n",
        "> ##### 奖励（reward） -r\n",
        "\n",
        "\n",
        "奖励（reward）是指在智能体执行一个动作之后，环境返回给智能体的一个数值.\n",
        "\n",
        "通常假设奖励是当前状态 $s$、当前动作 $a$、下一时刻状态 $s'$的函数，把奖励函数记作\n",
        "$$\n",
        "r(s, a, s′\n",
        ")\n",
        "$$\n",
        "\n",
        "\n",
        "有时假设奖励仅仅是 $s$ 和 $a$ 的函数，记作 \n",
        "$$\n",
        "r(s, a)\n",
        "$$\n",
        "\n",
        "我们总是假设奖励函数是\n",
        "有界的，即对于所有 $a ∈ A$ 和 $s$, $s' ∈ S$，有 \n",
        "$$\n",
        "|r(s, a, s' )| < ∞\n",
        "$$\n",
        "\n",
        "> ##### 状态转移（state transition)\n",
        "\n",
        "状态转移（state transition）是指智能体从当前 t 时刻的状态 $s$ 转移到下一个时刻状\n",
        "态为$s'$的过程。\n",
        "\n",
        "在超级玛丽的例子中，基于当前状态（屏幕上的画面），玛丽奥向上跳\n",
        "了一步，那么环境（即游戏程序）就会计算出新的状态（即下一帧画面）\n",
        "\n",
        "\n",
        "\n",
        ">  ##### 状态转移概率函数(state transition probability function)\n",
        "\n",
        "\n",
        "\n",
        "我们用状态转移概率函数（state transition\n",
        "probability function）来描述状态转移，记作:\n",
        "\n",
        "\n",
        "$$\n",
        "  P_t(s'|s,a)=P(S_{t+1}=s'|S_t=s,A_t =a)\n",
        "$$\n",
        "\n",
        "表示这个事件的概率：在当前状态$s$,智能体执行动作$a$,环境的状态变成$s'$\n",
        "\n",
        "\n",
        "> ##### 策略（policy)\n",
        "\n",
        "策略（policy）的意思是根据观测到的状态，如何做出决策,即如何从动作空间中选取一个动作。\n",
        "\n",
        "强化学习的目标就是得到一个策略函数，在每个时刻根据观测到的状态做出决策。\n",
        "\n",
        "*策略可以是确定性的，也可以是随机性的*\n",
        "\n",
        "*  ##### 随机策略\n",
        "\n",
        "把状态记作 $S$ 或 $s$，动作记作 $A$ 或 $a$，随机策略函数\n",
        "$\n",
        "π : S × A → [0, 1]\n",
        "$\n",
        "是一个概率密度函数：\n",
        "\n",
        "$$\n",
        "  \\pi(a|s)=P(A=a|S=s)\n",
        "$$\n",
        "\n",
        "策略函数的输入是状态 $s$ 和动作 $a$，输出是一个0到1之间的概率值。\n",
        "\n",
        "以超级玛丽为例，\n",
        "状态是游戏屏幕画面，把它作为策略函数的输入，策略函数可以告诉我每个动作的概率\n",
        "值：\n",
        "\n",
        "$$\n",
        "  π(左|s)=0.2,\n",
        "  π(右|s)=0.1\n",
        "$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J_j5lsFBnu8b"
      }
    }
  ]
}